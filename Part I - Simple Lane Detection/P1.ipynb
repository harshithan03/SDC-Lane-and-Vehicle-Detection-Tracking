{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Finding Lane Lines on the Road** \n",
    "Built by Galen Ballew in December 2016 through the Udacity Self Driving Car Engineer Nanodegree*\n",
    "***\n",
    "In this project, I built a computer vision pipeline for detecting lane lines and creating averaged and extrapolated boundary lines.\n",
    "The pipeline is as follows: \n",
    "\n",
    "1) Convert frame to grayscale  \n",
    "2) Create masks for yellow and white pixels  \n",
    "3) Apply a Gaussian smoothing  \n",
    "4) Apply a Canny edge detection  \n",
    "5) Create an additional mask to focus on the \"region of interest\" in front of the vehicle  \n",
    "6) Convert the points(i.e. pixels) in XY space to a line in Hough space  \n",
    "7) Where the lines in Hough space intersect (i.e. a point) a line exists in XY space  \n",
    "8) Using the extrema of the lines generated, create two averaged line  s\n",
    "9) Create two averaged lines across frames for a smooth video playback  \n",
    "10) Draw the lines to each frame\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<figure>\n",
    " <img src=\"line-segments-example.jpg\" width=\"380\" alt=\"Combined Image\" />\n",
    " <figcaption>\n",
    " <p></p> \n",
    " <p style=\"text-align: center;\"> Raw lines after conversion from Hough space </p> \n",
    " </figcaption>\n",
    "</figure>\n",
    " <p></p> \n",
    "<figure>\n",
    " <img src=\"laneLines_thirdPass.jpg\" width=\"380\" alt=\"Combined Image\" />\n",
    " <figcaption>\n",
    " <p></p> \n",
    " <p style=\"text-align: center;\"> Averaged lines for smooth playback</p> \n",
    " </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#importing some useful packages\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML\n",
    "import math\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def grayscale(img):\n",
    "    \"\"\"Applies the Grayscale transform\n",
    "    This will return an image with only one color channel\n",
    "    but NOTE: to see the returned image as grayscale\n",
    "    (assuming your grayscaled image is called 'gray')\n",
    "    you should call plt.imshow(gray, cmap='gray')\"\"\"\n",
    "    return cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    # Or use BGR2GRAY if you read an image with cv2.imread()\n",
    "    # return cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "def canny(img, low_threshold, high_threshold):\n",
    "    \"\"\"Applies the Canny transform\"\"\"\n",
    "    return cv2.Canny(img, low_threshold, high_threshold)\n",
    "\n",
    "def gaussian_blur(img, kernel_size):\n",
    "    \"\"\"Applies a Gaussian Noise kernel\"\"\"\n",
    "    return cv2.GaussianBlur(img, (kernel_size, kernel_size), 0)\n",
    "\n",
    "def region_of_interest(img, vertices):\n",
    "    \"\"\"\n",
    "    Applies an image mask.\n",
    "    \n",
    "    Only keeps the region of the image defined by the polygon\n",
    "    formed from `vertices`. The rest of the image is set to black.\n",
    "    \"\"\"\n",
    "    #defining a blank mask to start with\n",
    "    mask = np.zeros_like(img)   \n",
    "    \n",
    "    #defining a 3 channel or 1 channel color to fill the mask with depending on the input image\n",
    "    if len(img.shape) > 2:\n",
    "        channel_count = img.shape[2]  # i.e. 3 or 4 depending on your image\n",
    "        ignore_mask_color = (255,) * channel_count\n",
    "    else:\n",
    "        ignore_mask_color = 255\n",
    "        \n",
    "    #filling pixels inside the polygon defined by \"vertices\" with the fill color    \n",
    "    cv2.fillPoly(mask, vertices, ignore_mask_color)\n",
    "    \n",
    "    #returning the image only where mask pixels are nonzero\n",
    "    masked_image = cv2.bitwise_and(img, mask)\n",
    "    return masked_image\n",
    "\n",
    "#used below\n",
    "def get_slope(x1,y1,x2,y2):\n",
    "    return (y2-y1)/(x2-x1)\n",
    "\n",
    "#thick red lines \n",
    "def draw_lines(img, lines, color=[255, 0, 0], thickness=6):\n",
    "    \"\"\"workflow:\n",
    "    1) examine each individual line returned by hough & determine if it's in left or right lane by its slope\n",
    "    because we are working \"upside down\" with the array, the left lane will have a negative slope and right positive\n",
    "    2) track extrema\n",
    "    3) compute averages\n",
    "    4) solve for b intercept \n",
    "    5) use extrema to solve for points\n",
    "    6) smooth frames and cache\n",
    "    \"\"\"\n",
    "    global cache\n",
    "    global first_frame\n",
    "    y_global_min = img.shape[0] #min will be the \"highest\" y value, or point down the road away from car\n",
    "    y_max = img.shape[0]\n",
    "    l_slope, r_slope = [],[]\n",
    "    l_lane,r_lane = [],[]\n",
    "    det_slope = 0.4\n",
    "    α =0.2 \n",
    "    #i got this alpha value off of the forums for the weighting between frames.\n",
    "    #i understand what it does, but i dont understand where it comes from\n",
    "    #much like some of the parameters in the hough function\n",
    "    \n",
    "    for line in lines:\n",
    "        #1\n",
    "        for x1,y1,x2,y2 in line:\n",
    "            slope = get_slope(x1,y1,x2,y2)\n",
    "            if slope > det_slope:\n",
    "                r_slope.append(slope)\n",
    "                r_lane.append(line)\n",
    "            elif slope < -det_slope:\n",
    "                l_slope.append(slope)\n",
    "                l_lane.append(line)\n",
    "        #2\n",
    "        y_global_min = min(y1,y2,y_global_min)\n",
    "    \n",
    "    # to prevent errors in challenge video from dividing by zero\n",
    "    if((len(l_lane) == 0) or (len(r_lane) == 0)):\n",
    "        print ('no lane detected')\n",
    "        return 1\n",
    "        \n",
    "    #3\n",
    "    l_slope_mean = np.mean(l_slope,axis =0)\n",
    "    r_slope_mean = np.mean(r_slope,axis =0)\n",
    "    l_mean = np.mean(np.array(l_lane),axis=0)\n",
    "    r_mean = np.mean(np.array(r_lane),axis=0)\n",
    "    \n",
    "    if ((r_slope_mean == 0) or (l_slope_mean == 0 )):\n",
    "        print('dividing by zero')\n",
    "        return 1\n",
    "    \n",
    "   \n",
    "    \n",
    "    #4, y=mx+b -> b = y -mx\n",
    "    l_b = l_mean[0][1] - (l_slope_mean * l_mean[0][0])\n",
    "    r_b = r_mean[0][1] - (r_slope_mean * r_mean[0][0])\n",
    "    \n",
    "    #5, using y-extrema (#2), b intercept (#4), and slope (#3) solve for x using y=mx+b\n",
    "    # x = (y-b)/m\n",
    "    # these 4 points are our two lines that we will pass to the draw function\n",
    "    l_x1 = int((y_global_min - l_b)/l_slope_mean) \n",
    "    l_x2 = int((y_max - l_b)/l_slope_mean)   \n",
    "    r_x1 = int((y_global_min - r_b)/r_slope_mean)\n",
    "    r_x2 = int((y_max - r_b)/r_slope_mean)\n",
    "    \n",
    "    #6\n",
    "    if l_x1 > r_x1:\n",
    "        l_x1 = int((l_x1+r_x1)/2)\n",
    "        r_x1 = l_x1\n",
    "        l_y1 = int((l_slope_mean * l_x1 ) + l_b)\n",
    "        r_y1 = int((r_slope_mean * r_x1 ) + r_b)\n",
    "        l_y2 = int((l_slope_mean * l_x2 ) + l_b)\n",
    "        r_y2 = int((r_slope_mean * r_x2 ) + r_b)\n",
    "    else:\n",
    "        l_y1 = y_global_min\n",
    "        l_y2 = y_max\n",
    "        r_y1 = y_global_min\n",
    "        r_y2 = y_max\n",
    "      \n",
    "    current_frame = np.array([l_x1,l_y1,l_x2,l_y2,r_x1,r_y1,r_x2,r_y2],dtype =\"float32\")\n",
    "    \n",
    "    if first_frame == 1:\n",
    "        next_frame = current_frame        \n",
    "        first_frame = 0        \n",
    "    else :\n",
    "        prev_frame = cache\n",
    "        next_frame = (1-α)*prev_frame+α*current_frame\n",
    "             \n",
    "    cv2.line(img, (int(next_frame[0]), int(next_frame[1])), (int(next_frame[2]),int(next_frame[3])), color, thickness)\n",
    "    cv2.line(img, (int(next_frame[4]), int(next_frame[5])), (int(next_frame[6]),int(next_frame[7])), color, thickness)\n",
    "    \n",
    "    cache = next_frame\n",
    "    \n",
    "\n",
    "def hough_lines(img, rho, theta, threshold, min_line_len, max_line_gap):\n",
    "    \"\"\"\n",
    "    `img` should be the output of a Canny transform.\n",
    "        \n",
    "    Returns an image with hough lines drawn.\n",
    "    \"\"\"\n",
    "    lines = cv2.HoughLinesP(img, rho, theta, threshold, np.array([]), minLineLength=min_line_len, maxLineGap=max_line_gap)\n",
    "    line_img = np.zeros((img.shape[0], img.shape[1], 3), dtype=np.uint8)\n",
    "    draw_lines(line_img,lines)\n",
    "    return line_img\n",
    "\n",
    "# Python 3 has support for cool math symbols.\n",
    "\n",
    "def weighted_img(img, initial_img, α=0.8, β=1., λ=0.):\n",
    "    \"\"\"\n",
    "    `img` is the output of the hough_lines(), An image with lines drawn on it.\n",
    "    Should be a blank image (all black) with lines drawn on it.\n",
    "    \n",
    "    `initial_img` should be the image before any processing.\n",
    "    \n",
    "    The result image is computed as follows:\n",
    "    \n",
    "    initial_img * α + img * β + λ\n",
    "    NOTE: initial_img and img must be the same shape!\n",
    "    \"\"\"\n",
    "    return cv2.addWeighted(initial_img, α, img, β, λ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on Images\n",
    "\n",
    "Run on single still frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 51\u001b[0m\n\u001b[0;32m     49\u001b[0m first_frame \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     50\u001b[0m image \u001b[38;5;241m=\u001b[39m mpimg\u001b[38;5;241m.\u001b[39mimread(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_images/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39msource_img)\n\u001b[1;32m---> 51\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m mpimg\u001b[38;5;241m.\u001b[39mimsave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout_images/annotated_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39msource_img,processed)\n",
      "Cell \u001b[1;32mIn[3], line 44\u001b[0m, in \u001b[0;36mprocess_image\u001b[1;34m(image)\u001b[0m\n\u001b[0;32m     41\u001b[0m max_line_gap \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m180\u001b[39m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m#my hough values started closer to the values in the quiz, but got bumped up considerably for the challenge video\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m line_image \u001b[38;5;241m=\u001b[39m \u001b[43mhough_lines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroi_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrho\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_line_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_line_gap\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m result \u001b[38;5;241m=\u001b[39m weighted_img(line_image, image, α\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m, β\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.\u001b[39m, λ\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.\u001b[39m)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "Cell \u001b[1;32mIn[2], line 149\u001b[0m, in \u001b[0;36mhough_lines\u001b[1;34m(img, rho, theta, threshold, min_line_len, max_line_gap)\u001b[0m\n\u001b[0;32m    147\u001b[0m lines \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mHoughLinesP(img, rho, theta, threshold, np\u001b[38;5;241m.\u001b[39marray([]), minLineLength\u001b[38;5;241m=\u001b[39mmin_line_len, maxLineGap\u001b[38;5;241m=\u001b[39mmax_line_gap)\n\u001b[0;32m    148\u001b[0m line_img \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m3\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39muint8)\n\u001b[1;32m--> 149\u001b[0m \u001b[43mdraw_lines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlines\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m line_img\n",
      "Cell \u001b[1;32mIn[2], line 70\u001b[0m, in \u001b[0;36mdraw_lines\u001b[1;34m(img, lines, color, thickness)\u001b[0m\n\u001b[0;32m     65\u001b[0m α \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m \n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m#i got this alpha value off of the forums for the weighting between frames.\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m#i understand what it does, but i dont understand where it comes from\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m#much like some of the parameters in the hough function\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlines\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#1\u001b[39;49;00m\n\u001b[0;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43my1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\u001b[43my2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43mslope\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mget_slope\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43my1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\u001b[43my2\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "\n",
    "def process_image(image):\n",
    "    \n",
    "    global first_frame\n",
    "\n",
    "    gray_image = grayscale(image)\n",
    "    img_hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "    #hsv = [hue, saturation, value]\n",
    "    #more accurate range for yellow since it is not strictly black, white, r, g, or b\n",
    "\n",
    "    lower_yellow = np.array([20, 100, 100], dtype = \"uint8\")\n",
    "    upper_yellow = np.array([30, 255, 255], dtype=\"uint8\")\n",
    "\n",
    "    mask_yellow = cv2.inRange(img_hsv, lower_yellow, upper_yellow)\n",
    "    mask_white = cv2.inRange(gray_image, 200, 255)\n",
    "    mask_yw = cv2.bitwise_or(mask_white, mask_yellow)\n",
    "    mask_yw_image = cv2.bitwise_and(gray_image, mask_yw)\n",
    "\n",
    "    kernel_size = 5\n",
    "    gauss_gray = gaussian_blur(mask_yw_image,kernel_size)\n",
    "\n",
    "    #same as quiz values\n",
    "    low_threshold = 50\n",
    "    high_threshold = 150\n",
    "    canny_edges = canny(gauss_gray,low_threshold,high_threshold)\n",
    "\n",
    "    imshape = image.shape\n",
    "    lower_left = [imshape[1]/9,imshape[0]]\n",
    "    lower_right = [imshape[1]-imshape[1]/9,imshape[0]]\n",
    "    top_left = [imshape[1]/2-imshape[1]/8,imshape[0]/2+imshape[0]/10]\n",
    "    top_right = [imshape[1]/2+imshape[1]/8,imshape[0]/2+imshape[0]/10]\n",
    "    vertices = [np.array([lower_left,top_left,top_right,lower_right],dtype=np.int32)]\n",
    "    roi_image = region_of_interest(canny_edges, vertices)\n",
    "\n",
    "    #rho and theta are the distance and angular resolution of the grid in Hough space\n",
    "    #same values as quiz\n",
    "    rho = 4\n",
    "    theta = np.pi/180\n",
    "    #threshold is minimum number of intersections in a grid for candidate line to go to output\n",
    "    threshold = 30\n",
    "    min_line_len = 100\n",
    "    max_line_gap = 180\n",
    "    #my hough values started closer to the values in the quiz, but got bumped up considerably for the challenge video\n",
    "\n",
    "    line_image = hough_lines(roi_image, rho, theta, threshold, min_line_len, max_line_gap)\n",
    "    result = weighted_img(line_image, image, α=0.8, β=1., λ=0.)\n",
    "    return result\n",
    "\n",
    "for source_img in os.listdir(\"test_images/\"):\n",
    "    first_frame = 1\n",
    "    image = mpimg.imread(\"test_images/\"+source_img)\n",
    "    processed = process_image(image)\n",
    "    mpimg.imsave(\"out_images/annotated_\"+source_img,processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on Videos\n",
    "\n",
    "Apply the still frame processing to video feeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video white.mp4.\n",
      "Moviepy - Writing video white.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready white.mp4\n",
      "CPU times: total: 1.86 s\n",
      "Wall time: 3.8 s\n"
     ]
    }
   ],
   "source": [
    "first_frame = 1\n",
    "white_output = 'white.mp4'\n",
    "clip1 = VideoFileClip(\"solidWhiteRight.mp4\")\n",
    "white_clip = clip1.fl_image(process_image) #NOTE: this function expects color images!!\n",
    "%time white_clip.write_videofile(white_output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"960\" height=\"540\" controls>\n",
       "  <source src=\"white.mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(white_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video yellow.mp4.\n",
      "Moviepy - Writing video yellow.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready yellow.mp4\n",
      "CPU times: total: 6.22 s\n",
      "Wall time: 10.4 s\n"
     ]
    }
   ],
   "source": [
    "first_frame = 1\n",
    "yellow_output = 'yellow.mp4'\n",
    "clip2 = VideoFileClip('solidYellowLeft.mp4')\n",
    "yellow_clip = clip2.fl_image(process_image)\n",
    "%time yellow_clip.write_videofile(yellow_output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"960\" height=\"540\" controls>\n",
       "  <source src=\"yellow.mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(yellow_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflections\n",
    "\n",
    "\n",
    "1) create an ROI function that doesnt just use proportion of the frame to identify the polygon we are interested in. if you were driving a car of different height (think of how high up a semi cab is) or using a different camera, you'd always want the ROI to be accurate. what if the car is going up or down hill? what if there is another vehicle directly in front of the camera? what about rain, fog, and especially snow?! (these driving conditions will also have a great effect upon the color recognition, see 2) the best approach seems to be using lidar like sebastion thrun and the stanford team did for the great race. this is not as cost effective as a camera however.\n",
    "\n",
    "2) driving at nighttime. I don't have any footage of this, but different strength or colored headlights could affect the color thresholds, especially for the yellow lanes. any reflective lanes should be recognized as white, but that could be an issue. needs more testing. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Challenge\n",
    "\n",
    "This segment of video was a particular challenge due to the curve in the road. There were multiple intersections in Hough space that result in too many lines in XY. The averaged lines had to be modulated in order to be aware of the 'rolling extrema' of the other line (so that they don't cross)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video extra.mp4.\n",
      "Moviepy - Writing video extra.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:  28%|██▊       | 71/251 [00:01<00:04, 40.91it/s, now=None]"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot convert float NaN to integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m<timed eval>:1\u001b[0m\n",
      "File \u001b[1;32m<decorator-gen-73>:2\u001b[0m, in \u001b[0;36mwrite_videofile\u001b[1;34m(self, filename, fps, codec, bitrate, audio, audio_fps, preset, audio_nbytes, audio_codec, audio_bitrate, audio_bufsize, temp_audiofile, rewrite_audio, remove_temp, write_logfile, verbose, threads, ffmpeg_params, logger)\u001b[0m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\moviepy\\decorators.py:54\u001b[0m, in \u001b[0;36mrequires_duration\u001b[1;34m(f, clip, *a, **k)\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mduration\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not set\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<decorator-gen-72>:2\u001b[0m, in \u001b[0;36mwrite_videofile\u001b[1;34m(self, filename, fps, codec, bitrate, audio, audio_fps, preset, audio_nbytes, audio_codec, audio_bitrate, audio_bufsize, temp_audiofile, rewrite_audio, remove_temp, write_logfile, verbose, threads, ffmpeg_params, logger)\u001b[0m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\moviepy\\decorators.py:135\u001b[0m, in \u001b[0;36muse_clip_fps_by_default\u001b[1;34m(f, clip, *a, **k)\u001b[0m\n\u001b[0;32m    130\u001b[0m new_a \u001b[38;5;241m=\u001b[39m [fun(arg) \u001b[38;5;28;01mif\u001b[39;00m (name\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfps\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m arg\n\u001b[0;32m    131\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m (arg, name) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(a, names)]\n\u001b[0;32m    132\u001b[0m new_kw \u001b[38;5;241m=\u001b[39m {k: fun(v) \u001b[38;5;28;01mif\u001b[39;00m k\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfps\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m v\n\u001b[0;32m    133\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m (k,v) \u001b[38;5;129;01min\u001b[39;00m k\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m--> 135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnew_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnew_kw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<decorator-gen-71>:2\u001b[0m, in \u001b[0;36mwrite_videofile\u001b[1;34m(self, filename, fps, codec, bitrate, audio, audio_fps, preset, audio_nbytes, audio_codec, audio_bitrate, audio_bufsize, temp_audiofile, rewrite_audio, remove_temp, write_logfile, verbose, threads, ffmpeg_params, logger)\u001b[0m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\moviepy\\decorators.py:22\u001b[0m, in \u001b[0;36mconvert_masks_to_RGB\u001b[1;34m(f, clip, *a, **k)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clip\u001b[38;5;241m.\u001b[39mismask:\n\u001b[0;32m     21\u001b[0m     clip \u001b[38;5;241m=\u001b[39m clip\u001b[38;5;241m.\u001b[39mto_RGB()\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\moviepy\\video\\VideoClip.py:300\u001b[0m, in \u001b[0;36mVideoClip.write_videofile\u001b[1;34m(self, filename, fps, codec, bitrate, audio, audio_fps, preset, audio_nbytes, audio_codec, audio_bitrate, audio_bufsize, temp_audiofile, rewrite_audio, remove_temp, write_logfile, verbose, threads, ffmpeg_params, logger)\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m make_audio:\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maudio\u001b[38;5;241m.\u001b[39mwrite_audiofile(audiofile, audio_fps,\n\u001b[0;32m    294\u001b[0m                                audio_nbytes, audio_bufsize,\n\u001b[0;32m    295\u001b[0m                                audio_codec, bitrate\u001b[38;5;241m=\u001b[39maudio_bitrate,\n\u001b[0;32m    296\u001b[0m                                write_logfile\u001b[38;5;241m=\u001b[39mwrite_logfile,\n\u001b[0;32m    297\u001b[0m                                verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    298\u001b[0m                                logger\u001b[38;5;241m=\u001b[39mlogger)\n\u001b[1;32m--> 300\u001b[0m \u001b[43mffmpeg_write_video\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcodec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mbitrate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbitrate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mpreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    303\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mwrite_logfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_logfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    304\u001b[0m \u001b[43m                   \u001b[49m\u001b[43maudiofile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maudiofile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    306\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mffmpeg_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mffmpeg_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remove_temp \u001b[38;5;129;01mand\u001b[39;00m make_audio:\n\u001b[0;32m    310\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(audiofile):\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\moviepy\\video\\io\\ffmpeg_writer.py:220\u001b[0m, in \u001b[0;36mffmpeg_write_video\u001b[1;34m(clip, filename, fps, codec, bitrate, preset, withmask, write_logfile, audiofile, verbose, threads, ffmpeg_params, logger)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m FFMPEG_VideoWriter(filename, clip\u001b[38;5;241m.\u001b[39msize, fps, codec \u001b[38;5;241m=\u001b[39m codec,\n\u001b[0;32m    214\u001b[0m                             preset\u001b[38;5;241m=\u001b[39mpreset, bitrate\u001b[38;5;241m=\u001b[39mbitrate, logfile\u001b[38;5;241m=\u001b[39mlogfile,\n\u001b[0;32m    215\u001b[0m                             audiofile\u001b[38;5;241m=\u001b[39maudiofile, threads\u001b[38;5;241m=\u001b[39mthreads,\n\u001b[0;32m    216\u001b[0m                             ffmpeg_params\u001b[38;5;241m=\u001b[39mffmpeg_params) \u001b[38;5;28;01mas\u001b[39;00m writer:\n\u001b[0;32m    218\u001b[0m     nframes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(clip\u001b[38;5;241m.\u001b[39mduration\u001b[38;5;241m*\u001b[39mfps)\n\u001b[1;32m--> 220\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclip\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_frames\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwith_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mfps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muint8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwithmask\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    223\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mclip\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\moviepy\\Clip.py:472\u001b[0m, in \u001b[0;36mClip.iter_frames\u001b[1;34m(self, fps, with_times, logger, dtype)\u001b[0m\n\u001b[0;32m    470\u001b[0m logger \u001b[38;5;241m=\u001b[39m proglog\u001b[38;5;241m.\u001b[39mdefault_bar_logger(logger)\n\u001b[0;32m    471\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m logger\u001b[38;5;241m.\u001b[39miter_bar(t\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mduration, \u001b[38;5;241m1.0\u001b[39m\u001b[38;5;241m/\u001b[39mfps)):\n\u001b[1;32m--> 472\u001b[0m     frame \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    473\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (frame\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m dtype):\n\u001b[0;32m    474\u001b[0m         frame \u001b[38;5;241m=\u001b[39m frame\u001b[38;5;241m.\u001b[39mastype(dtype)\n",
      "File \u001b[1;32m<decorator-gen-29>:2\u001b[0m, in \u001b[0;36mget_frame\u001b[1;34m(self, t)\u001b[0m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\moviepy\\decorators.py:89\u001b[0m, in \u001b[0;36mpreprocess_args.<locals>.wrapper\u001b[1;34m(f, *a, **kw)\u001b[0m\n\u001b[0;32m     85\u001b[0m new_a \u001b[38;5;241m=\u001b[39m [fun(arg) \u001b[38;5;28;01mif\u001b[39;00m (name \u001b[38;5;129;01min\u001b[39;00m varnames) \u001b[38;5;28;01melse\u001b[39;00m arg\n\u001b[0;32m     86\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m (arg, name) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(a, names)]\n\u001b[0;32m     87\u001b[0m new_kw \u001b[38;5;241m=\u001b[39m {k: fun(v) \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m varnames \u001b[38;5;28;01melse\u001b[39;00m v\n\u001b[0;32m     88\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m (k,v) \u001b[38;5;129;01min\u001b[39;00m kw\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m---> 89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnew_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnew_kw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\moviepy\\Clip.py:93\u001b[0m, in \u001b[0;36mClip.get_frame\u001b[1;34m(self, t)\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m frame\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\moviepy\\Clip.py:136\u001b[0m, in \u001b[0;36mClip.fl.<locals>.<lambda>\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    133\u001b[0m     apply_to \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m#mf = copy(self.make_frame)\u001b[39;00m\n\u001b[1;32m--> 136\u001b[0m newclip \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_make_frame(\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_frame\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m keep_duration:\n\u001b[0;32m    139\u001b[0m     newclip\u001b[38;5;241m.\u001b[39mduration \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\moviepy\\video\\VideoClip.py:490\u001b[0m, in \u001b[0;36mVideoClip.fl_image.<locals>.<lambda>\u001b[1;34m(gf, t)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;124;03mModifies the images of a clip by replacing the frame\u001b[39;00m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;124;03m`get_frame(t)` by another frame,  `image_func(get_frame(t))`\u001b[39;00m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    489\u001b[0m apply_to \u001b[38;5;241m=\u001b[39m apply_to \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[1;32m--> 490\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfl(\u001b[38;5;28;01mlambda\u001b[39;00m gf, t: \u001b[43mimage_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m, apply_to)\n",
      "Cell \u001b[1;32mIn[3], line 44\u001b[0m, in \u001b[0;36mprocess_image\u001b[1;34m(image)\u001b[0m\n\u001b[0;32m     41\u001b[0m max_line_gap \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m180\u001b[39m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m#my hough values started closer to the values in the quiz, but got bumped up considerably for the challenge video\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m line_image \u001b[38;5;241m=\u001b[39m \u001b[43mhough_lines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroi_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrho\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_line_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_line_gap\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m result \u001b[38;5;241m=\u001b[39m weighted_img(line_image, image, α\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m, β\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.\u001b[39m, λ\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.\u001b[39m)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "Cell \u001b[1;32mIn[2], line 149\u001b[0m, in \u001b[0;36mhough_lines\u001b[1;34m(img, rho, theta, threshold, min_line_len, max_line_gap)\u001b[0m\n\u001b[0;32m    147\u001b[0m lines \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mHoughLinesP(img, rho, theta, threshold, np\u001b[38;5;241m.\u001b[39marray([]), minLineLength\u001b[38;5;241m=\u001b[39mmin_line_len, maxLineGap\u001b[38;5;241m=\u001b[39mmax_line_gap)\n\u001b[0;32m    148\u001b[0m line_img \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m3\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39muint8)\n\u001b[1;32m--> 149\u001b[0m \u001b[43mdraw_lines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlines\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m line_img\n",
      "Cell \u001b[1;32mIn[2], line 107\u001b[0m, in \u001b[0;36mdraw_lines\u001b[1;34m(img, lines, color, thickness)\u001b[0m\n\u001b[0;32m    102\u001b[0m r_b \u001b[38;5;241m=\u001b[39m r_mean[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m (r_slope_mean \u001b[38;5;241m*\u001b[39m r_mean[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m#5, using y-extrema (#2), b intercept (#4), and slope (#3) solve for x using y=mx+b\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# x = (y-b)/m\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# these 4 points are our two lines that we will pass to the draw function\u001b[39;00m\n\u001b[1;32m--> 107\u001b[0m l_x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_global_min\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43ml_b\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43ml_slope_mean\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m    108\u001b[0m l_x2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m((y_max \u001b[38;5;241m-\u001b[39m l_b)\u001b[38;5;241m/\u001b[39ml_slope_mean)   \n\u001b[0;32m    109\u001b[0m r_x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m((y_global_min \u001b[38;5;241m-\u001b[39m r_b)\u001b[38;5;241m/\u001b[39mr_slope_mean)\n",
      "\u001b[1;31mValueError\u001b[0m: cannot convert float NaN to integer"
     ]
    }
   ],
   "source": [
    "first_frame = 1\n",
    "challenge_output = 'extra.mp4'\n",
    "clip2 = VideoFileClip('challenge.mp4')\n",
    "challenge_clip = clip2.fl_image(process_image)\n",
    "%time challenge_clip.write_videofile(challenge_output, audio=False)\n",
    "#TODO how do we make curved ROI and curved lines? we need some calculus up in this bizzzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"960\" height=\"540\" controls>\n",
       "  <source src=\"extra.mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:  29%|██▉       | 73/251 [00:11<00:04, 40.91it/s, now=None]"
     ]
    }
   ],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(challenge_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
